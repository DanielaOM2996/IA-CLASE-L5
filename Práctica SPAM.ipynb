{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correos electrónicos SPAM: Un enfoque con Procesamiento de Lenguaje Natural."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alumna: Daniela Olivas Mendoza."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los correos electrónicos no deseados en su bandeja de entrada son molestos ya que perturban la rutina del usuario. Es por eso que las cuentas de correo electrónico ya tiene un filtro spam. Dado que es una de las palicaciones del PLN más utilizadas, vamos a ver como se desarrolló un filtro de spam simple para correos electrónicos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importamos las famosas librerías\n",
    "from functools import reduce\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "import string \n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jorge\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Insertando los datos\n",
    "full_corpus = pd.read_csv('SMSSpamCollection.tsv', sep='/t', header= None, names=['label', 'msg_body'])\n",
    "\n",
    "#Separando los mensajes en 'ham' y 'spam'\n",
    "ham_text = []\n",
    "spam_text = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los N-gramos se usan para modelar el lenguaje de función de la predicción de palabras, es decir, predice la siguiente palabra de una oración de palabras N1 anteriores. Bigram es la secuencia de 2 palabras de N-gramos que predice la siguiente palabra de una oración usando la palabra anterior. En lugar de considerar la historia completa de una oración o una secuencia de palabras en particular, un modelo de bigram puede ser ocupado en términos de uuna aproximación de la historia al ocupar uuna historia limitada.\n",
    "\n",
    "La identificación de un mensaje como 'ham' o 'spam' es una tarea de clasificación ya que la variable de destino tiene valores discretos que son \"ham\" o \"spam\". En esta práctica, se usa el modelo bigram, aunque existen muchas técnicas avanzadas que s epueden utilizar para este propósito. Para utilizar el modelo bigram para asignar un mensaje dado como \"spam\" o \"ham\", hay varios pasos que deben lograrse:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Inspección y separación de mensajes en las categorías \"ham\" y \"spam\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nicialmente, el conjunto de datos debe inspeccionarse para ocuparlo y abordarlo para lograr la tarea. El formato de los datos dados, la cantidad de datos proporcionados, la naturaleza de los datos se incluyen en esta inspección para identificar la mejor aproximación posible para la tarea.\n",
    "\n",
    "El corpus de mensajes dado ha marcado cada mensaje como ham o spam. Además, hay 5568 mensajes en uun DataFrame escrito en inglés que no son objetos nulos. Por lo tanto, el archivo tsv se puede leer usando DataFrame en Python para clasificar esos mensajes de acuerdo con el indicador dado.\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_msgs():\n",
    "    for index, column in full_corpus.iterrowa():\n",
    "        label = column[0]\n",
    "        message_text = column[1]\n",
    "        if label == 'ham':\n",
    "            ham_text.append(message_text)\n",
    "        elif label == \"spam\":\n",
    "            spam_text.append(message_text)\n",
    "\n",
    "            separate_msgs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Procesamiento de texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El preprocesamiento es la tarea de realizar los pasos de preparación en el corpus de texto sin formato para completar de manera edificente una extracción de texto o procesamiento de lenguaje natural o cualquier tarea que incluya texto sin formato. El preprocesamiento de texto consta de varios pasos, aunque algunos de ellos pueden aplicarse a una tarea en particular debido a la naturaleza del conjunto de datos disponible.\n",
    "\n",
    "En esta tarea, el preprocesamiento de texto incliye los siguientes pasos de acuerdo con el conjunto de datos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminación de signos de puntuación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminación de signos de puntuación de los mensajes del correo elec.\n",
    "def remove_msg_punctuations(email_msg):\n",
    "    puntuation_removed_msg = \"\".join([word for word in email_msg if word not in string.punctuation])\n",
    "    return punctuation_removed_msg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convertir minúsculas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convertir a minúsculas: la conversión de todos los caracteres del texto en un contexto común, como los soportes en minúsculas, impide identificar dos palabras de manera diferente donde una está en minúscula y la otra no. Por ejemplo, \"Primero\" y \"primero\" deben identificarse como iguales, por lo tanto, poner en minúscula todos los caracteres facilita la tarea. Además, las palabras de detención también están em minúsculas, por lo que esto también haría posible eliminar palabras de detención más adelante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Tokenizing es la tarea de dividir el texto en partes significativas, es decir, tokens que incluyen oraciones y palabras. Un token se puede considerar como una instancia de una secuencia de caracteres en uun texto particular que se agrupan para proporcionar una unidad semantica util para su posterior procesamiento. En esta tarea, la tokenización de palabras se utiliza cambiando espacios en blanco entre palabras como delimitador. Esto se logra en Python, usando expresiones regulares para dividir una cadena en subcadenas con la función split(), que es un tokenizador básico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convierte el texto en minúsculas y tokenizing de palabras.\n",
    "\n",
    "def tokenize_into_words(text):\n",
    "    tokens = re.split('/W+', text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Palabras lemantizantes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La derivación es el proceso de eliminar afijos (sufijos, prefijos, infijos, circunfijos) de una palabra para obtener su raíz de palabra. Aunque la lematización está relacionada con la derivación, difiere ya que la lemantización puede capturar formas canónicas basadas en el lema de una palabra. La lematización ocupa un vocabulario y un análisis morfológico de las palabras que lo hacen más rápido y preciso que la derivación. WordNetLemmatizer ha lograo la lematización en lenguaje Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatizing\n",
    "word_lemmatizer = WordNetLemmatizer()\n",
    "def lemmatization(tokenized_words):\n",
    "    lemmatized_text = [word_lemmatizer.lemmatize(word)for word in tokenized_words]\n",
    "    return \" \".join(lemmatized_text)\n",
    "\n",
    "def preprocessing_msg(corpus):\n",
    "    categorized_text = pd.DataFrame(corpus)\n",
    "    categorized_text['non_punc_message_body'] = categorized_text[0].apply(lambda msg: remove_msg_punctuations(msg))\n",
    "    categorized_text['tokenized_msg_body'] = categorized_text['non_punc_message_body'].apply(lambda msg: tokenize_into_words(msg.lower()))\n",
    "    categorized_text['lemmatized_msg_word'] = categorized_text['tokenized_msg_body'].apply(lambda word_list: lemmatization(word_list))\n",
    "    return  categorized_text['lemmatized_msg_word']\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extracción de características."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

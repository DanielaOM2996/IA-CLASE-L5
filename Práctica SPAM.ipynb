{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correos electrónicos SPAM: Un enfoque con Procesamiento de Lenguaje Natural."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alumna: Daniela Olivas Mendoza."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los correos electrónicos no deseados en su bandeja de entrada son molestos ya que perturban la rutina del usuario. Es por eso que las cuentas de correo electrónico ya tiene un filtro spam. Dado que es una de las palicaciones del PLN más utilizadas, vamos a ver como se desarrolló un filtro de spam simple para correos electrónicos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importamos las famosas librerías\n",
    "from functools import reduce\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "import string \n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jorge\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Insertando los datos\n",
    "full_corpus = pd.read_csv('SMSSpamCollection.tsv', sep='/t', header= None, names=['label', 'msg_body'])\n",
    "\n",
    "#Separando los mensajes en 'ham' y 'spam'\n",
    "ham_text = []\n",
    "spam_text = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los N-gramos se usan para modelar el lenguaje de función de la predicción de palabras, es decir, predice la siguiente palabra de una oración de palabras N1 anteriores. Bigram es la secuencia de 2 palabras de N-gramos que predice la siguiente palabra de una oración usando la palabra anterior. En lugar de considerar la historia completa de una oración o una secuencia de palabras en particular, un modelo de bigram puede ser ocupado en términos de uuna aproximación de la historia al ocupar uuna historia limitada.\n",
    "\n",
    "La identificación de un mensaje como 'ham' o 'spam' es una tarea de clasificación ya que la variable de destino tiene valores discretos que son \"ham\" o \"spam\". En esta práctica, se usa el modelo bigram, aunque existen muchas técnicas avanzadas que s epueden utilizar para este propósito. Para utilizar el modelo bigram para asignar un mensaje dado como \"spam\" o \"ham\", hay varios pasos que deben lograrse:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Inspección y separación de mensajes en las categorías \"ham\" y \"spam\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nicialmente, el conjunto de datos debe inspeccionarse para ocuparlo y abordarlo para lograr la tarea. El formato de los datos dados, la cantidad de datos proporcionados, la naturaleza de los datos se incluyen en esta inspección para identificar la mejor aproximación posible para la tarea.\n",
    "\n",
    "El corpus de mensajes dado ha marcado cada mensaje como ham o spam. Además, hay 5568 mensajes en uun DataFrame escrito en inglés que no son objetos nulos. Por lo tanto, el archivo tsv se puede leer usando DataFrame en Python para clasificar esos mensajes de acuerdo con el indicador dado.\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_msgs():\n",
    "    for index, column in full_corpus.iterrowa():\n",
    "        label = column[0]\n",
    "        message_text = column[1]\n",
    "        if label == 'ham':\n",
    "            ham_text.append(message_text)\n",
    "        elif label == \"spam\":\n",
    "            spam_text.append(message_text)\n",
    "\n",
    "            separate_msgs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Procesamiento de texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El preprocesamiento es la tarea de realizar los pasos de preparación en el corpus de texto sin formato para completar de manera edificente una extracción de texto o procesamiento de lenguaje natural o cualquier tarea que incluya texto sin formato. El preprocesamiento de texto consta de varios pasos, aunque algunos de ellos pueden aplicarse a una tarea en particular debido a la naturaleza del conjunto de datos disponible.\n",
    "\n",
    "En esta tarea, el preprocesamiento de texto incliye los siguientes pasos de acuerdo con el conjunto de datos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminación de signos de puntuación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminación de signos de puntuación de los mensajes del correo elec.\n",
    "def remove_msg_punctuations(email_msg):\n",
    "    puntuation_removed_msg = \"\".join([word for word in email_msg if word not in string.punctuation])\n",
    "    return punctuation_removed_msg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convertir minúsculas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convertir a minúsculas: la conversión de todos los caracteres del texto en un contexto común, como los soportes en minúsculas, impide identificar dos palabras de manera diferente donde una está en minúscula y la otra no. Por ejemplo, \"Primero\" y \"primero\" deben identificarse como iguales, por lo tanto, poner en minúscula todos los caracteres facilita la tarea. Además, las palabras de detención también están em minúsculas, por lo que esto también haría posible eliminar palabras de detención más adelante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Tokenizing es la tarea de dividir el texto en partes significativas, es decir, tokens que incluyen oraciones y palabras. Un token se puede considerar como una instancia de una secuencia de caracteres en uun texto particular que se agrupan para proporcionar una unidad semantica util para su posterior procesamiento. En esta tarea, la tokenización de palabras se utiliza cambiando espacios en blanco entre palabras como delimitador. Esto se logra en Python, usando expresiones regulares para dividir una cadena en subcadenas con la función split(), que es un tokenizador básico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convierte el texto en minúsculas y tokenizing de palabras.\n",
    "\n",
    "def tokenize_into_words(text):\n",
    "    tokens = re.split('/W+', text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Palabras lemantizantes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La derivación es el proceso de eliminar afijos (sufijos, prefijos, infijos, circunfijos) de una palabra para obtener su raíz de palabra. Aunque la lematización está relacionada con la derivación, difiere ya que la lemantización puede capturar formas canónicas basadas en el lema de una palabra. La lematización ocupa un vocabulario y un análisis morfológico de las palabras que lo hacen más rápido y preciso que la derivación. WordNetLemmatizer ha lograo la lematización en lenguaje Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatizing\n",
    "word_lemmatizer = WordNetLemmatizer()\n",
    "def lemmatization(tokenized_words):\n",
    "    lemmatized_text = [word_lemmatizer.lemmatize(word)for word in tokenized_words]\n",
    "    return \" \".join(lemmatized_text)\n",
    "\n",
    "def preprocessing_msg(corpus):\n",
    "    categorized_text = pd.DataFrame(corpus)\n",
    "    categorized_text['non_punc_message_body'] = categorized_text[0].apply(lambda msg: remove_msg_punctuations(msg))\n",
    "    categorized_text['tokenized_msg_body'] = categorized_text['non_punc_message_body'].apply(lambda msg: tokenize_into_words(msg.lower()))\n",
    "    categorized_text['lemmatized_msg_word'] = categorized_text['tokenized_msg_body'].apply(lambda word_list: lemmatization(word_list))\n",
    "    return  categorized_text['lemmatized_msg_word']\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extracción de características."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Después de la etapa de preprocesamiento, las características deben extraerse del texto. Las características son las unidades la tarea de clasificación, y las bigrams son las caracteristicas en esta tarea de clasificacion de mensajes. Los bigrams o las caracteristicas se extraen de la tarea de clasificación de mensajes. Los bigrams o las características se extraen del texto preprocesado. Inicialmente, los unigramas se adquieren, y luego esos unigramas se utilizan para obtener los unigramas en cada corpus (\"ham\" y \"spam\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracción de características. Ejemplo: n-grams\n",
    "def feature_extraction(preprocessed_text):\n",
    "    bigrams = []\n",
    "    unigrams_list =[]\n",
    "    for msg in preprocessed_text:\n",
    "        #agregando end of and start of al mensaje\n",
    "        msg = '<s>' +msg +'</s'\n",
    "        unigrams_list.append(msg.split())\n",
    "        unigrams = [uni_lists for sub_list in unigrams_list for uni_list in sub_list]\n",
    "        bigrams.extend(nltk.bigrams(unigrams))\n",
    "        return bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Eliminación de Stop Words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay ciertas palabras en un idioma (se utiliza inglés en la practica) que son necesarias para una oracion o una secuencia de palabras, aunque no contribuyen al significado de una frase considerada. La biblioteca del Kit de herramientas de lenguaje natural (NLTK) en Python proporciona palabras de detención comunes en algunos idiomas.\n",
    "\n",
    "\n",
    "En lugar de eliminar las palabras de detención en el paso de preprocesamiento, se realiza después de extraer las caracteristicas del corpus para evitar la ausencia de bigrams con palabras de una parada (('use', 'your'), ('to', 'win')) al adquirir las funciones, ya que tienen un impacto en el resultado final de la aplicación. Las palabras de detención se pueden ignorar en esta recuperaciónde información orientada a palabras clave debido a su efecto en la precisión de la recuperación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminando bigrams solo con stop words\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "def filter_stopwords_bigrams(bigram_list):\n",
    "    filtered_bigrams = []\n",
    "    for bigram in bigram_list:\n",
    "        if bigram[0] in stopwords and bigrams[1] in stopwords:\n",
    "            continue\n",
    "            filtered_bigrams.append(bigram)\n",
    "            return filtered_bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Obtener distribución de frecuencia de características."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La distribución de frecuencia se utiliza para obtener la frecuencia de aparición de cada elemento de vocabulario en un texto determinado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adquiriendo la frecuencia de características\n",
    "\n",
    "def ham_bigram_feature_frequency():\n",
    "    # Frecuencia de características para mensajes ham\n",
    "    ham_bigram = feature_extraction(preprocessing_msgs(ham_text))\n",
    "    ham_bigram_frequency = nltk.FreqDist(filter_stopwords_bigrams(ham_bigrams))\n",
    "    return ham_bigram_frequency\n",
    "\n",
    "def spam_bigram_feature_frequency():\n",
    "    # Frecuencia de caracteristicas para mensajes spam\n",
    "    spam_bigram = feature_extraction(preprocessing_msgs(spam_text))\n",
    "    spam_bigram_frequency = nltk.FreqDist(filter_stopwords_bigrams(spam_bigrams))\n",
    "    return spam_bigram_frequency\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Construyendo un modelo para predicción."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo para clasificar dado como 'ham' o 'spam'se ha abordado calculando las probabilidades de bigram dentro de cada corpus. Inicialmente, el mensaje dado debe procesarse previamente para avanzar con la clasificacion, incluida la eliminacion de signos de puntuacion, el cambio de todos los caracteres a minusculas, la tokenizacion y la lematizacion. Luego, los bigrams se extraen del texto preprocesado para calcular finalmente la probabilidad de que el texto esté en cada corpus \"ham\" o 'spam\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocessing_msgs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-c9fd4ff4cad7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\\"'\u001b[0m \u001b[1;33m+\u001b[0m\u001b[0mmessage\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;34m'\\\" es un mensaje Spam'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m \u001b[0mbigram_probability\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Click here,  ..to win an iphone 11 pro max'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m \u001b[0mbigram_probability\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Win a brand new car '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-c9fd4ff4cad7>\u001b[0m in \u001b[0;36mbigram_probability\u001b[1;34m(message)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mbigrams_for_msg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbigrams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlemmatized_msg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;31m# Eliminamos stop words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mham_unigrams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfeature_extraction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreprocessing_msgs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mham_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0mspam_unigrams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfeature_extraction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreprocessing_msgs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspam_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;31m# Frecuencias de bigrams extraidas\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'preprocessing_msgs' is not defined"
     ]
    }
   ],
   "source": [
    "# Calculando probabilidades del bigram\n",
    "def bigram_probability(message):\n",
    "    probability_h = 1\n",
    "    probability_s = 1\n",
    "    # Preprocesando los mensaje de entrada\n",
    "    punc_removed_message = \"\".join(word for word in message if word not in string.punctuation)\n",
    "    punc_removed_message = '<s> ' +punc_removed_message +' </s>'\n",
    "    tokenized_msg = re.split('\\s+', punc_removed_message)\n",
    "    lemmatized_msg = [word_lemmatizer.lemmatize(word)for word in tokenized_msg]\n",
    "    # bigrams para el mensaje\n",
    "    bigrams_for_msg = list(nltk.bigrams(lemmatized_msg))\n",
    "    # Eliminamos stop words\n",
    "    ham_unigrams = [word for word in feature_extraction(preprocessing_msgs(ham_text)) if word not in stopwords]\n",
    "    spam_unigrams = [word for word in feature_extraction(preprocessing_msgs(spam_text)) if word not in stopwords]\n",
    "    # Frecuencias de bigrams extraidas\n",
    "    ham_frequency = ham_bigram_feature_frequency()\n",
    "    spam_frequency  = spam_bigram_feature_frequency()\n",
    "    print('========================== Calculando Probabilidades ==========================')\n",
    "    print('----------- Frecuencias Ham ------------')\n",
    "    for bigram in bigrams_for_msg:\n",
    "        # probabilidad de la primera palabra en bigram\n",
    "        ham_probability_denominator = 0\n",
    "        # probabilidad de bigram (suavizado) \n",
    "        ham_probability_of_bigram = ham_frequency[bigram] + 1\n",
    "        print(bigram, ' ocurre ', ham_probability_of_bigram)\n",
    "        for (first_unigram, second_unigram) in filter_stopwords_bigrams(ham_unigrams):\n",
    "            ham_probability_denominator += 1\n",
    "            if(first_unigram == bigram[0]):\n",
    "                ham_probability_denominator += ham_frequency[first_unigram, second_unigram]\n",
    "        probability = ham_probability_of_bigram / ham_probability_denominator\n",
    "        probability_h *= probability\n",
    "    print('\\n')\n",
    "    print('----------- Frecuencias Spam ------------')\n",
    "    for bigram in bigrams_for_msg:\n",
    "        # probabilidad de la primera palabra en bigram\n",
    "        spam_probability_denominator = 0\n",
    "        # probabilidad de bigram (suavizado) \n",
    "        spam_probability_of_bigram = spam_frequency[bigram] + 1\n",
    "        print(bigram, ' ocurre ', spam_probability_of_bigram)\n",
    "        for (first_unigram, second_unigram) in filter_stopwords_bigrams(spam_unigrams):\n",
    "            spam_probability_denominator += 1\n",
    "            if(first_unigram == bigram[0]):\n",
    "                spam_probability_denominator += spam_frequency[first_unigram, second_unigram]\n",
    "        probability = spam_probability_of_bigram / spam_probability_denominator\n",
    "        probability_s *= probability\n",
    "    print('\\n')\n",
    "    print('Probabilidad Ham: ' +str(probability_h))\n",
    "    print('Probabildiad Spam: ' +str(probability_s))\n",
    "    print('\\n')\n",
    "    if(probability_h >= probability_s):\n",
    "        print('\\\"' +message +'\\\" es un mensaje Ham')\n",
    "    else:\n",
    "        print('\\\"' +message +'\\\" es un mensaje Spam')\n",
    "    print('\\n')\n",
    "bigram_probability('Click here,  ..to win an iphone 11 pro max')\n",
    "bigram_probability('Win a brand new car ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
